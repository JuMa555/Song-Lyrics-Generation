{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TWDJMiTZCGDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxKFXde2AZrV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Dense, Lambda, Embedding, Flatten\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Song-Lyrics-Generation/data/processed/dataset.csv\", index_col=[0])\n"
      ],
      "metadata": {
        "id": "55b1Ut_sCCdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(n=10000, random_state=42)"
      ],
      "metadata": {
        "id": "zFaQuy6nzsNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract lyrics from the dataset\n",
        "lyrics = df['Lyric'].tolist()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[\"Lyric\"].astype(str).str.lower())\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "tokenized_sentences = tokenizer.texts_to_sequences(df[\"Lyric\"].astype(str))"
      ],
      "metadata": {
        "id": "lkHVAT9LzNVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def generate_sequences(tokenized_sentences):\n",
        "    for i in tqdm(tokenized_sentences, desc=\"Generating sequences\"):\n",
        "        for t in range(1, len(i)):\n",
        "            n_gram_sequence = i[: t + 1]\n",
        "            yield n_gram_sequence"
      ],
      "metadata": {
        "id": "WxVMtj3NzG_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_generator = generate_sequences(tokenized_sentences)\n",
        "\n",
        "# Find the maximum sequence length.\n",
        "max_sequence_len = max(len(seq) for seq in tqdm(sequence_generator, desc=\"Calculating max sequence length\"))\n",
        "\n",
        "# Create a new generator for sequences.\n",
        "sequence_generator = generate_sequences(tokenized_sentences)\n",
        "\n",
        "# Pad sequences in smaller batches to save memory.\n",
        "batch_size = 1000\n",
        "padded_sequences = []\n",
        "\n",
        "for batch in tqdm(iter(lambda: list(sequence_generator)[:batch_size], []), desc=\"Padding sequences\"):\n",
        "    padded_sequences.extend(keras.preprocessing.sequence.pad_sequences(batch, maxlen=max_sequence_len, padding=\"pre\"))\n",
        "\n",
        "input_sequences = np.array(padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akicy5vZzfiA",
        "outputId": "9fbf5344-0de4-492c-e198-987eec9bfadc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating max sequence length: 0it [00:00, ?it/s]\n",
            "Calculating max sequence length: 105569it [00:00, 1055630.60it/s]\n",
            "Calculating max sequence length: 211133it [00:00, 1029861.58it/s]\n",
            "Calculating max sequence length: 328508it [00:00, 1094663.37it/s]\n",
            "Calculating max sequence length: 446317it [00:00, 1127313.40it/s]\n",
            "Calculating max sequence length: 559400it [00:00, 1128556.90it/s]\n",
            "Calculating max sequence length: 672317it [00:00, 1086885.08it/s]\n",
            "Calculating max sequence length: 782743it [00:00, 1092391.54it/s]\n",
            "Calculating max sequence length: 893005it [00:00, 1095570.79it/s]\n",
            "Calculating max sequence length: 1009560it [00:00, 1117145.44it/s]\n",
            "Calculating max sequence length: 1126881it [00:01, 1134283.64it/s]\n",
            "Calculating max sequence length: 1240421it [00:01, 1106703.67it/s]\n",
            "Calculating max sequence length: 1356501it [00:01, 1122791.79it/s]\n",
            "Calculating max sequence length: 1473776it [00:01, 1137684.15it/s]\n",
            "Calculating max sequence length: 1595357it [00:01, 1161015.86it/s]\n",
            "Calculating max sequence length: 1711588it [00:01, 1161311.42it/s]\n",
            "Calculating max sequence length: 1827810it [00:01, 1127789.67it/s]\n",
            "Calculating max sequence length: 1940849it [00:01, 1116218.88it/s]\n",
            "Calculating max sequence length: 2058522it [00:01, 1133938.30it/s]\n",
            "Calculating max sequence length: 2178733it [00:01, 1154043.89it/s]\n",
            "Calculating max sequence length: 2296521it [00:02, 1161099.28it/s]\n",
            "Calculating max sequence length: 2412750it [00:02, 1144552.51it/s]\n",
            "Calculating max sequence length: 2527330it [00:02, 1059351.00it/s]\n",
            "Generating sequences: 100%|██████████| 10000/10000 [00:02<00:00, 4231.92it/s]\n",
            "Calculating max sequence length: 2634714it [00:02, 1112826.20it/s]\n",
            "Padding sequences: 0it [00:00, ?it/s]\n",
            "Generating sequences:   0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            "Generating sequences:   1%|▏         | 147/10000 [00:00<00:06, 1431.16it/s]\u001b[A\n",
            "Generating sequences:   3%|▎         | 291/10000 [00:00<00:18, 530.76it/s] \u001b[A\n",
            "Generating sequences:   5%|▍         | 471/10000 [00:00<00:11, 827.04it/s]\u001b[A\n",
            "Generating sequences:   7%|▋         | 653/10000 [00:00<00:08, 1075.18it/s]\u001b[A\n",
            "Generating sequences:   8%|▊         | 808/10000 [00:00<00:07, 1199.71it/s]\u001b[A\n",
            "Generating sequences:  10%|▉         | 958/10000 [00:01<00:16, 562.78it/s] \u001b[A\n",
            "Generating sequences:  11%|█▏        | 1126/10000 [00:01<00:12, 724.05it/s]\u001b[A\n",
            "Generating sequences:  13%|█▎        | 1274/10000 [00:01<00:10, 854.40it/s]\u001b[A\n",
            "Generating sequences:  14%|█▍        | 1435/10000 [00:01<00:08, 1003.74it/s]\u001b[A\n",
            "Generating sequences:  16%|█▌        | 1577/10000 [00:02<00:18, 449.56it/s] \u001b[A\n",
            "Generating sequences:  17%|█▋        | 1724/10000 [00:02<00:14, 566.12it/s]\u001b[A\n",
            "Generating sequences:  19%|█▊        | 1864/10000 [00:02<00:11, 681.16it/s]\u001b[A\n",
            "Generating sequences:  20%|██        | 2001/10000 [00:02<00:10, 795.69it/s]\u001b[A\n",
            "Generating sequences:  21%|██▏       | 2142/10000 [00:02<00:08, 905.36it/s]\u001b[A\n",
            "Generating sequences:  23%|██▎       | 2303/10000 [00:02<00:07, 1055.99it/s]\u001b[A\n",
            "Generating sequences:  24%|██▍       | 2442/10000 [00:03<00:20, 369.10it/s] \u001b[A\n",
            "Generating sequences:  26%|██▌       | 2596/10000 [00:04<00:15, 483.82it/s]\u001b[A\n",
            "Generating sequences:  28%|██▊       | 2778/10000 [00:04<00:11, 648.03it/s]\u001b[A\n",
            "Generating sequences:  29%|██▉       | 2939/10000 [00:04<00:08, 790.91it/s]\u001b[A\n",
            "Generating sequences:  31%|███       | 3083/10000 [00:04<00:07, 877.63it/s]\u001b[A\n",
            "Generating sequences:  32%|███▏      | 3246/10000 [00:04<00:06, 1024.43it/s]\u001b[A\n",
            "Generating sequences:  34%|███▍      | 3391/10000 [00:04<00:06, 1098.84it/s]\u001b[A\n",
            "Generating sequences:  35%|███▌      | 3533/10000 [00:05<00:21, 302.80it/s] \u001b[A\n",
            "Generating sequences:  37%|███▋      | 3668/10000 [00:05<00:16, 386.15it/s]\u001b[A\n",
            "Generating sequences:  38%|███▊      | 3781/10000 [00:06<00:13, 458.23it/s]\u001b[A\n",
            "Generating sequences:  39%|███▉      | 3891/10000 [00:06<00:11, 516.40it/s]\u001b[A\n",
            "Generating sequences:  40%|███▉      | 3993/10000 [00:06<00:10, 577.38it/s]\u001b[A\n",
            "Generating sequences:  41%|████      | 4091/10000 [00:06<00:09, 616.96it/s]\u001b[A\n",
            "Generating sequences:  42%|████▏     | 4203/10000 [00:06<00:08, 700.59it/s]\u001b[A\n",
            "Generating sequences:  43%|████▎     | 4298/10000 [00:06<00:07, 752.38it/s]\u001b[A\n",
            "Generating sequences:  44%|████▍     | 4393/10000 [00:06<00:07, 794.29it/s]\u001b[A\n",
            "Generating sequences:  45%|████▍     | 4492/10000 [00:06<00:06, 840.98it/s]\u001b[A\n",
            "Generating sequences:  46%|████▌     | 4588/10000 [00:06<00:06, 848.14it/s]\u001b[A\n",
            "Generating sequences:  47%|████▋     | 4702/10000 [00:07<00:05, 921.55it/s]\u001b[A\n",
            "Generating sequences:  48%|████▊     | 4801/10000 [00:09<00:37, 138.82it/s]\u001b[A\n",
            "Generating sequences:  49%|████▉     | 4897/10000 [00:09<00:27, 183.63it/s]\u001b[A\n",
            "Generating sequences:  50%|████▉     | 4975/10000 [00:09<00:22, 226.04it/s]\u001b[A\n",
            "Generating sequences:  51%|█████     | 5071/10000 [00:09<00:16, 294.54it/s]\u001b[A\n",
            "Generating sequences:  52%|█████▏    | 5163/10000 [00:09<00:13, 368.00it/s]\u001b[A\n",
            "Generating sequences:  53%|█████▎    | 5283/10000 [00:09<00:09, 487.71it/s]\u001b[A\n",
            "Generating sequences:  54%|█████▍    | 5381/10000 [00:09<00:08, 571.53it/s]\u001b[A\n",
            "Generating sequences:  55%|█████▍    | 5478/10000 [00:10<00:07, 610.79it/s]\u001b[A\n",
            "Generating sequences:  56%|█████▌    | 5568/10000 [00:10<00:06, 667.23it/s]\u001b[A\n",
            "Generating sequences:  57%|█████▋    | 5659/10000 [00:10<00:06, 719.40it/s]\u001b[A\n",
            "Generating sequences:  57%|█████▋    | 5748/10000 [00:10<00:05, 759.41it/s]\u001b[A\n",
            "Generating sequences:  59%|█████▊    | 5857/10000 [00:10<00:04, 844.75it/s]\u001b[A\n",
            "Generating sequences:  60%|█████▉    | 5952/10000 [00:10<00:04, 862.35it/s]\u001b[A\n",
            "Generating sequences:  61%|██████    | 6057/10000 [00:10<00:04, 913.94it/s]\u001b[A\n",
            "Generating sequences:  62%|██████▏   | 6216/10000 [00:10<00:03, 1103.86it/s]\u001b[A\n",
            "Generating sequences:  64%|██████▍   | 6386/10000 [00:10<00:02, 1273.91it/s]\u001b[A\n",
            "Generating sequences:  65%|██████▌   | 6518/10000 [00:13<00:19, 182.00it/s] \u001b[A\n",
            "Generating sequences:  66%|██████▋   | 6647/10000 [00:13<00:13, 243.43it/s]\u001b[A\n",
            "Generating sequences:  68%|██████▊   | 6806/10000 [00:13<00:09, 342.23it/s]\u001b[A\n",
            "Generating sequences:  70%|██████▉   | 6983/10000 [00:13<00:06, 477.75it/s]\u001b[A\n",
            "Generating sequences:  71%|███████▏  | 7131/10000 [00:13<00:04, 595.54it/s]\u001b[A\n",
            "Generating sequences:  73%|███████▎  | 7276/10000 [00:13<00:03, 715.65it/s]\u001b[A\n",
            "Generating sequences:  74%|███████▍  | 7441/10000 [00:13<00:02, 870.49it/s]\u001b[A\n",
            "Generating sequences:  76%|███████▌  | 7587/10000 [00:13<00:02, 967.37it/s]\u001b[A\n",
            "Generating sequences:  77%|███████▋  | 7744/10000 [00:13<00:02, 1095.62it/s]\u001b[A\n",
            "Generating sequences:  79%|███████▉  | 7891/10000 [00:13<00:01, 1180.43it/s]\u001b[A\n",
            "Generating sequences:  81%|████████  | 8082/10000 [00:14<00:01, 1349.66it/s]\u001b[A\n",
            "Generating sequences:  82%|████████▏ | 8240/10000 [00:14<00:01, 1371.86it/s]\u001b[A\n",
            "Generating sequences:  84%|████████▍ | 8424/10000 [00:14<00:01, 1494.07it/s]\u001b[A\n",
            "Generating sequences:  86%|████████▌ | 8586/10000 [00:17<00:07, 183.85it/s] \u001b[A\n",
            "Generating sequences:  87%|████████▋ | 8736/10000 [00:17<00:05, 243.33it/s]\u001b[A\n",
            "Generating sequences:  89%|████████▉ | 8878/10000 [00:17<00:03, 313.94it/s]\u001b[A\n",
            "Generating sequences:  90%|█████████ | 9018/10000 [00:17<00:02, 400.23it/s]\u001b[A\n",
            "Generating sequences:  92%|█████████▏| 9168/10000 [00:17<00:01, 512.42it/s]\u001b[A\n",
            "Generating sequences:  94%|█████████▎| 9354/10000 [00:17<00:00, 683.75it/s]\u001b[A\n",
            "Generating sequences:  95%|█████████▌| 9506/10000 [00:17<00:00, 687.76it/s]\u001b[A\n",
            "Generating sequences:  96%|█████████▋| 9637/10000 [00:17<00:00, 784.87it/s]\u001b[A\n",
            "Generating sequences:  98%|█████████▊| 9781/10000 [00:17<00:00, 903.87it/s]\u001b[A\n",
            "Generating sequences: 100%|██████████| 10000/10000 [00:18<00:00, 552.95it/s]\n",
            "Padding sequences: 1it [00:18, 18.98s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to a fixed length\n",
        "maxlen = 1000  # Set a reasonable maximum length\n",
        "padded_sequences = pad_sequences(sequences, padding='post', dtype='int32', truncating='post', maxlen=maxlen)\n"
      ],
      "metadata": {
        "id": "aDLXp5SvDJ4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 1000  # Set your desired maximum length\n",
        "truncated_sequences = [sequence[:max_length] for sequence in padded_sequences]\n"
      ],
      "metadata": {
        "id": "ZLXTJfLGDrcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = max(len(seq) for seq in input_sequences)\n",
        "maxlen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy0EElaSDS0t",
        "outputId": "be9d34f7-0d2a-4d21-bc8d-a0d3102fdc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4489"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(truncated_sequences)"
      ],
      "metadata": {
        "id": "gyJ-VcqQDxj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test = train_test_split(input_sequences, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7R_YQL-1ETpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define VAE parameters\n",
        "latent_dim = 32\n",
        "intermediate_dim = 256\n",
        "epsilon_std = 1.0\n",
        "embedding_dim = 200\n",
        "num_latent_vars = 3"
      ],
      "metadata": {
        "id": "_CCm1rsYEUob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reparameterization trick\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim), mean=0., stddev=epsilon_std)\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "AWQCC-eTEa0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "inputs = Input(shape=(maxlen,))\n",
        "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=maxlen)(inputs)\n",
        "flatten_layer = Flatten()(embedding_layer)\n",
        "h = Dense(intermediate_dim, activation='relu')(flatten_layer)\n",
        "z_mean = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "\n",
        "# Sample z using reparameterization trick\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
      ],
      "metadata": {
        "id": "PPaMISFXykw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "decoder_h = Dense(intermediate_dim, activation='relu')\n",
        "decoder_mean = Dense(maxlen, activation='sigmoid')\n",
        "\n",
        "h_decoded = decoder_h(z)\n",
        "x_decoded_mean = decoder_mean(h_decoded)"
      ],
      "metadata": {
        "id": "YRFgJnD-FFjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras import backend as K\n",
        "\n",
        "class CustomVariationalLayer(keras.layers.Layer):\n",
        "    def vae_loss(self, x, x_decoded_mean, z_mean, z_log_var):\n",
        "        xent_loss = maxlen * binary_crossentropy(x, x_decoded_mean)\n",
        "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0]\n",
        "        x_decoded_mean = inputs[1]\n",
        "        z_mean = inputs[2]\n",
        "        z_log_var = inputs[3]\n",
        "        loss = self.vae_loss(x, x_decoded_mean, z_mean, z_log_var)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3V99z4tAFQx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# Instantiate VAE model\n",
        "y = CustomVariationalLayer()([inputs, x_decoded_mean, z_mean, z_log_var])\n",
        "vae = Model(inputs, y)\n",
        "# Assuming you are using MSE as the loss function\n",
        "vae.compile(optimizer=Adam(clipvalue=1.0, learning_rate = 0.00001), loss=None)\n"
      ],
      "metadata": {
        "id": "aYzl5Ue0F9cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the summary of your VAE model to see the layer names\n",
        "vae.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdbBrjWNX6O2",
        "outputId": "ed82df0f-8d78-47ce-ffa4-83951d20398e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 4489)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 4489, 200)            8314600   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 897800)               0         ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 256)                  2298370   ['flatten[0][0]']             \n",
            "                                                          56                                      \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 32)                   8224      ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 32)                   8224      ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 32)                   0         ['dense_1[0][0]',             \n",
            "                                                                     'dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 256)                  8448      ['lambda[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 4489)                 1153673   ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " custom_variational_layer_3  (None, 4489)                 0         ['input_2[0][0]',             \n",
            "  (CustomVariationalLayer)                                           'dense_4[0][0]',             \n",
            "                                                                     'dense_1[0][0]',             \n",
            "                                                                     'dense_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 239330225 (912.97 MB)\n",
            "Trainable params: 239330225 (912.97 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import Callback\n",
        "\n",
        "class DebugCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Get the values at the end of each epoch\n",
        "        x_values = self.validation_data[0]\n",
        "        x_decoded_mean_values = self.model.predict(x_values)\n",
        "        z_mean_values, z_log_var_values = self.model.get_layer('custom_variational_layer_1').predict(x_values)\n",
        "\n",
        "        # Print or log the values\n",
        "        print(\"x:\", x_values)\n",
        "        print(\"x_decoded_mean:\", x_decoded_mean_values)\n",
        "        print(\"z_mean:\", z_mean_values)\n",
        "        print(\"z_log_var:\", z_log_var_values)"
      ],
      "metadata": {
        "id": "dztjJIYqWIi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "S6J0789dgUcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = vae.fit(X_train, epochs=10, batch_size=32, shuffle=True, validation_data=(X_test, None), callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtCcdxEjK_Ei",
        "outputId": "e2c60790-e2e5-4eec-f2f2-deaad00f82bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "25/25 [==============================] - 9s 296ms/step - loss: -5441053.0000 - val_loss: -4776094.0000\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 4s 170ms/step - loss: -6206898.5000 - val_loss: -6353220.5000\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 3s 135ms/step - loss: -6872452.0000 - val_loss: -6316979.5000\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 4s 154ms/step - loss: -7228686.0000 - val_loss: -6415738.5000\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 3s 114ms/step - loss: -8187629.0000 - val_loss: -6290669.5000\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 3s 105ms/step - loss: -9278574.0000 - val_loss: -5973985.5000\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 3s 139ms/step - loss: -9044991.0000 - val_loss: -7852519.0000\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 5s 206ms/step - loss: -10466345.0000 - val_loss: -8529614.0000\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 3s 141ms/step - loss: -10435073.0000 - val_loss: -9005307.0000\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 2s 95ms/step - loss: -11705098.0000 - val_loss: -6697683.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_lyric_vae(vae, tokenizer, latent_dim=32, maxlen=4489):\n",
        "    # Generate new lyrics\n",
        "    new_lyric_vector = np.random.normal(size=(1, 4489))\n",
        "    decoded_lyric = vae.predict(new_lyric_vector)\n",
        "\n",
        "    # Inverse transform the decoded lyric back to text\n",
        "    decoded_lyric_text = tokenizer.sequences_to_texts([decoded_lyric.argmax(axis=-1) + 1])[0]\n",
        "\n",
        "    return decoded_lyric_text\n",
        "\n",
        "# Example usage:\n",
        "generated_lyric = generate_lyric_vae(vae, tokenizer)\n",
        "print(\"Generated Lyric:\\n\", generated_lyric)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et3QeHsg2pVe",
        "outputId": "f23532de-c9b4-4932-9ab4-ff59ab8475a8"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n",
            "Generated Lyric:\n",
            " bringin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_lyric_vae_with_seed(vae, tokenizer, seed_text, latent_dim=32, maxlen=4489, n_words=50, temperature=1.0):\n",
        "    # Tokenize the seed text\n",
        "    seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    print(seed_sequence)\n",
        "    seed_padded = pad_sequences([seed_sequence], maxlen=maxlen, padding='pre', truncating='pre')\n",
        "\n",
        "    # Generate new lyrics word by word\n",
        "    generated_lyric = seed_text\n",
        "    for _ in range(n_words):\n",
        "        # Predict the next word based on the seed text\n",
        "        predicted = vae.predict(seed_padded, verbose=0)[0]\n",
        "\n",
        "        # Sample from the modified distribution using temperature\n",
        "        predicted = np.log(predicted) / temperature\n",
        "        exp_preds = np.exp(predicted)\n",
        "        normalized_preds = exp_preds / np.sum(exp_preds)\n",
        "        predicted_index = np.argmax(np.random.multinomial(1, normalized_preds, 1))\n",
        "\n",
        "        # Convert the index back to a word\n",
        "        output_word = tokenizer.index_word.get(predicted_index, \"\")\n",
        "\n",
        "        # Update the seed text for the next iteration\n",
        "        seed_text += \" \" + output_word\n",
        "        seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        seed_padded = pad_sequences([seed_sequence], maxlen=maxlen, padding='pre', truncating='pre')\n",
        "\n",
        "        # Update the generated lyric\n",
        "        generated_lyric += \" \" + output_word\n",
        "\n",
        "    return generated_lyric\n",
        "\n",
        "# Example usage:\n",
        "seed_text = \"I want to\"\n",
        "generated_lyric = generate_lyric_vae_with_seed(vae, tokenizer, seed_text, n_words= 5, temperature=10)\n",
        "print(\"Generated Lyric:\\n\", generated_lyric)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkGJCRaE4Q_G",
        "outputId": "fbed8d14-4e3a-44e8-a23b-59b6de133f3c"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 63, 4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-191-b485f023f7f9>:16: RuntimeWarning: divide by zero encountered in log\n",
            "  predicted = np.log(predicted) / temperature\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Lyric:\n",
            " I want to cheers disrespect cheers disrespect floss\n"
          ]
        }
      ]
    }
  ]
}