{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aaliyah</td>\n",
       "      <td>Mmm yeh yeh \\n A special smile \\n A certain to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>steven-tyler</td>\n",
       "      <td>Right now, nothing else matters \\n You and me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the-beatles</td>\n",
       "      <td>Gonna tell Aunt Mary 'bout Uncle John, \\n He s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>norah-jones</td>\n",
       "      <td>I'm lonely \\n 'Cause I'm looking at pictures o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dexys-midnight-runners</td>\n",
       "      <td>No, I don't want sympathy, \\n I just want some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  Artist  \\\n",
       "0           0                 aaliyah   \n",
       "1           1            steven-tyler   \n",
       "2           2             the-beatles   \n",
       "3           3             norah-jones   \n",
       "4           4  dexys-midnight-runners   \n",
       "\n",
       "                                               Lyric  \n",
       "0  Mmm yeh yeh \\n A special smile \\n A certain to...  \n",
       "1  Right now, nothing else matters \\n You and me ...  \n",
       "2  Gonna tell Aunt Mary 'bout Uncle John, \\n He s...  \n",
       "3  I'm lonely \\n 'Cause I'm looking at pictures o...  \n",
       "4  No, I don't want sympathy, \\n I just want some...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df[\"Lyric\"].astype(str).str.lower())\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "tokenized_sentences = tokenizer.texts_to_sequences(df[\"Lyric\"].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slash sequences into n gram sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(tokenized_sentences):\n",
    "    for i in tqdm(tokenized_sentences, desc=\"Generating sequences\"):\n",
    "        for t in range(1, len(i)):\n",
    "            n_gram_sequence = i[: t + 1]\n",
    "            yield n_gram_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_generator = generate_sequences(tokenized_sentences)\n",
    "\n",
    "# Find the maximum sequence length.\n",
    "max_sequence_len = max(len(seq) for seq in tqdm(sequence_generator, desc=\"Calculating max sequence length\"))\n",
    "\n",
    "# Create a new generator for sequences.\n",
    "sequence_generator = generate_sequences(tokenized_sentences)\n",
    "\n",
    "# Pad sequences in smaller batches to save memory.\n",
    "batch_size = 1000\n",
    "padded_sequences = []\n",
    "\n",
    "for batch in tqdm(iter(lambda: list(sequence_generator)[:batch_size], []), desc=\"Padding sequences\"):\n",
    "    padded_sequences.extend(keras.preprocessing.sequence.pad_sequences(batch, maxlen=max_sequence_len, padding=\"pre\"))\n",
    "\n",
    "input_sequences = np.array(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create predictors and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, labels = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "dataset = dict()\n",
    "dataset[\"features\"] = X\n",
    "dataset[\"labels\"] = y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
