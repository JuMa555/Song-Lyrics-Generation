{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7149104,"sourceType":"datasetVersion","datasetId":4127458}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport torch\nimport wandb\nimport csv\nimport re\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, trange","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize WandB\nwandb.init(project='gpt-2', entity='tomislav-krog')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/lyrics/dataset.csv')\ndf = df[(df['Artist'] == 'alicia-keys')]\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop the songs with lyrics too long\ndf = df[df['Lyric'].apply(lambda x: len(x.split(' ')) < 450)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_lyrics(lyrics):\n    # Removing bracketed text\n    pattern = r'\\[.*?\\]'\n    lyrics = re.sub(pattern, '', lyrics)\n\n    # Removing newline symbols\n    lyrics = re.sub('\\n', '', lyrics)\n    \n    # Removing specific parenthesized text\n    pattern = r'\\((chorus|CHORUS|verse|VERSE|intro|INTRO)(.*?)\\)'\n    lyrics = re.sub(pattern, '', lyrics)\n\n    # Function to resolve special symbols\n    def special_symbols_resolver(s):\n        replacements = {\n            'à': 'a', 'á': 'a', 'â': 'a', 'ã': 'a', 'ä': 'a',\n            'ç': 'c',\n            'ö': 'o',\n            'ú': 'u', 'ü': 'u',\n            'œ': 'oe',\n            'Â': 'A',\n            '‰': '', '™': '', '´': '', '·': '', '¦': '', '': '', '': '',\n            '˜': '', '“': '', '†': '', '…': '', '′': '', '″': '', '�': '',\n            'í': 'i', 'é': 'e', 'ï': 'i', 'ó': 'o', ';': ',', '‘': '\\'', '’': '\\'', ':': ',', 'е': 'e'\n        }\n        for symbol, replacement in replacements.items():\n            s = s.replace(symbol, replacement)\n        return s\n\n    # Apply the special symbols resolver\n    lyrics = special_symbols_resolver(lyrics)\n\n    # Further cleaning\n    replace_with_space = ['\\u2005', '\\u200b', '\\u205f', '\\xa0', '-']\n    remove_list = ['\\)', '\\(', '–', '\"', '”', '\"', '\\[.*\\]', '.*\\|.*', '—', '(VERSE)', '(CHORUS ONE)']\n\n    for string in remove_list:\n        lyrics = re.sub(string, '', lyrics)\n    for string in replace_with_space:\n        lyrics = re.sub(string, ' ', lyrics)\n\n    return lyrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Lyric'] = df['Lyric'].apply(preprocess_lyrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_size = int(len(df)*0.05)\n\n# test set\ntest_set = df.sample(n = test_set_size)\ndf = df.loc[~df.index.isin(test_set.index)]\ntest_set = test_set.reset_index()\ndf = df.reset_index()\n\n# keep last 30 words in a new column, then remove them from original column\ntest_set['TrueFinalLyric'] = test_set['Lyric'].str.split().str[-30:].apply(' '.join)\ntest_set['Lyric'] = test_set['Lyric'].str.split().str[:-30].apply(' '.join)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SongLyrics(Dataset):  \n    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n        self.lyrics = []\n\n        for row in df['Lyric']:\n          self.lyrics.append(torch.tensor(\n                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n            ))               \n        if truncate:\n            self.lyrics = self.lyrics[:20000]\n        self.lyrics_count = len(self.lyrics)\n        \n    def __len__(self):\n        return self.lyrics_count\n\n    def __getitem__(self, item):\n        return self.lyrics[item]\n    \ndataset = SongLyrics(df['Lyric'], truncate=True, gpt2_type=\"gpt2\")   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# accumulated batch size\ndef pack_tensor(new_tensor, packed_tensor, max_seq_len):\n    if packed_tensor is None:\n        return new_tensor, True, None\n    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n        return packed_tensor, False, new_tensor\n    else:\n        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n        return packed_tensor, True, None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(\n    dataset, model, tokenizer,\n    batch_size=16, epochs=15, lr=2e-5,\n    max_seq_len=400, warmup_steps=200,\n    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n    test_mode=False, save_model_on_epoch=False,\n):\n\n    acc_steps = 100\n    device = torch.device(\"cuda\")\n    model = model.cuda()\n    model.train()\n\n    optimizer = AdamW(model.parameters(), lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n    )\n\n    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n    total_loss = 0\n    accumulating_batch_count = 0\n    input_tensor = None\n\n    for epoch in range(epochs):\n        print(f\"Training epoch {epoch}\")\n\n        epoch_loss = 0  # Reset loss for each epoch\n\n        for idx, entry in tqdm(enumerate(train_dataloader)):\n            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n\n            if carry_on and idx != len(train_dataloader) - 1:\n                continue\n\n            input_tensor = input_tensor.to(device)\n            outputs = model(input_tensor, labels=input_tensor)\n            loss = outputs[0]\n            loss.backward()\n\n            epoch_loss += loss.item()  # Accumulate the loss\n\n            if (accumulating_batch_count % batch_size) == 0:\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                model.zero_grad()\n\n            accumulating_batch_count += 1\n            input_tensor = None\n\n        # Log the average loss per epoch\n        wandb.log({\"epoch\": epoch, \"loss\": epoch_loss / len(train_dataloader)})\n\n        if save_model_on_epoch:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n            )\n\n        print(f\"Epoch {epoch} loss: {epoch_loss / len(train_dataloader)}\")\n\n    wandb.finish()\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train(dataset, model, tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(\n    model,\n    tokenizer,\n    prompt,\n    entry_count=10,\n    entry_length=30, #maximum number of words\n    top_p=0.8,\n    temperature=1.,\n):\n    model.eval()\n    generated_num = 0\n    generated_list = []\n\n    filter_value = -float(\"Inf\")\n\n    with torch.no_grad():\n\n        for entry_idx in trange(entry_count):\n\n            entry_finished = False\n            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n\n            for i in range(entry_length):\n                outputs = model(generated, labels=generated)\n                loss, logits = outputs[:2]\n                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n                    ..., :-1\n                ].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n                logits[:, indices_to_remove] = filter_value\n\n                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n                generated = torch.cat((generated, next_token), dim=1)\n\n                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n                    entry_finished = True\n\n                if entry_finished:\n\n                    generated_num = generated_num + 1\n\n                    output_list = list(generated.squeeze().numpy())\n                    output_text = tokenizer.decode(output_list)\n                    generated_list.append(output_text)\n                    break\n            \n            if not entry_finished:\n              output_list = list(generated.squeeze().numpy())\n              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n              generated_list.append(output_text)\n                \n    return generated_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_generation(test_data):\n  generated_lyrics = []\n  for i in range(len(test_data)):\n    x = generate(model.to('cpu'), tokenizer, test_data['Lyric'][i], entry_count=1)\n    generated_lyrics.append(x)\n  return generated_lyrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate the lyrics\ngenerated_lyrics = text_generation(test_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set['Lyric'][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set['TrueFinalLyric'][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_lyrics[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}