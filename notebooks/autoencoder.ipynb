{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/processed/dataset.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['Lyric'].astype(str))\n",
    "sequences = tokenizer.texts_to_sequences(data['Lyric'].astype(str))\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "sequences_padded = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(sequences_padded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder_model(max_sequence_length, total_words):\n",
    "    inputs = Input(shape=(max_sequence_length,))\n",
    "    encoded = Embedding(total_words, 128, input_length=max_sequence_length)(inputs)\n",
    "    encoded = LSTM(128)(encoded)\n",
    "\n",
    "    decoded = RepeatVector(max_sequence_length)(encoded)\n",
    "    decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "    decoded = Dense(total_words, activation='softmax')(decoded)\n",
    "\n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "autoencoder_model = create_autoencoder_model(max_sequence_length, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 121s 5s/step - loss: 7.3967 - accuracy: 0.6979 - val_loss: 4.5396 - val_accuracy: 0.7093\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 110s 4s/step - loss: 2.9501 - accuracy: 0.7274 - val_loss: 2.5713 - val_accuracy: 0.7093\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 111s 4s/step - loss: 2.3656 - accuracy: 0.7274 - val_loss: 2.5020 - val_accuracy: 0.7093\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 110s 4s/step - loss: 2.3277 - accuracy: 0.7274 - val_loss: 2.5030 - val_accuracy: 0.7093\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 110s 4s/step - loss: 2.3245 - accuracy: 0.7274 - val_loss: 2.5042 - val_accuracy: 0.7093\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 110s 4s/step - loss: 2.3171 - accuracy: 0.7274 - val_loss: 2.4901 - val_accuracy: 0.7093\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 110s 4s/step - loss: 2.2963 - accuracy: 0.7274 - val_loss: 2.4676 - val_accuracy: 0.7093\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 110s 4s/step - loss: 2.2714 - accuracy: 0.7274 - val_loss: 2.4418 - val_accuracy: 0.7093\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 111s 4s/step - loss: 2.2385 - accuracy: 0.7274 - val_loss: 2.4051 - val_accuracy: 0.7093\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 115s 5s/step - loss: 2.1926 - accuracy: 0.7274 - val_loss: 2.3577 - val_accuracy: 0.7093\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 115s 5s/step - loss: 2.1537 - accuracy: 0.7274 - val_loss: 2.3254 - val_accuracy: 0.7093\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 122s 5s/step - loss: 2.1192 - accuracy: 0.7274 - val_loss: 2.3046 - val_accuracy: 0.7093\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 124s 5s/step - loss: 2.0998 - accuracy: 0.7274 - val_loss: 2.2905 - val_accuracy: 0.7093\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 142s 6s/step - loss: 2.0814 - accuracy: 0.7274 - val_loss: 2.2831 - val_accuracy: 0.7093\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 145s 6s/step - loss: 2.0678 - accuracy: 0.7274 - val_loss: 2.2886 - val_accuracy: 0.7093\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 141s 6s/step - loss: 2.0660 - accuracy: 0.7274 - val_loss: 2.2767 - val_accuracy: 0.7093\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 139s 6s/step - loss: 2.0715 - accuracy: 0.7274 - val_loss: 2.2731 - val_accuracy: 0.7093\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 144s 6s/step - loss: 2.0526 - accuracy: 0.7274 - val_loss: 2.2809 - val_accuracy: 0.7093\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 144s 6s/step - loss: 2.0488 - accuracy: 0.7274 - val_loss: 2.2738 - val_accuracy: 0.7093\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 153s 6s/step - loss: 2.0454 - accuracy: 0.7274 - val_loss: 2.2771 - val_accuracy: 0.7093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b620194fa0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_model.fit(X_train, X_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jurica\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "autoencoder_model.save(\"autoencoder_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(seed_text, model, tokenizer, max_sequence_length, num_words_to_generate):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(num_words_to_generate):\n",
    "        seed_sequence = tokenizer.texts_to_sequences([generated_text])\n",
    "        padded_sequence = pad_sequences(seed_sequence, maxlen=max_sequence_length, padding='post')\n",
    "        encoded_text = model.predict(padded_sequence)\n",
    "        decoded_sequence = tokenizer.sequences_to_texts(encoded_text.argmax(axis=-1))\n",
    "        generated_text = \" \".join(decoded_sequence)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "1/1 [==============================] - 0s 287ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 262ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "Pjesma generirana na temelju riječi 'blue ocean':\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_word = 'blue ocean'\n",
    "generated_poem = generate_poem(seed_word, autoencoder_model, tokenizer, max_sequence_length, 10)\n",
    "\n",
    "# Ispisivanje generirane pjesme\n",
    "print(f\"Pjesma generirana na temelju riječi '{seed_word}':\\n{generated_poem}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
