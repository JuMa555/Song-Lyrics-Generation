{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoGh7hUehxOV"
      },
      "outputs": [],
      "source": [
        "#%pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n",
        "#%pip install bitsandbytes einops wandb -Uqqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkStffVvhjdQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import glob\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import wandb\n",
        "from peft import get_peft_model, PeftConfig, PeftModel, LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, GenerationConfig\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "from huggingface_hub import notebook_login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKVczUiSStOS"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSaQPkvDLlgW"
      },
      "outputs": [],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project='tiny-llama', entity='tomislav-krog')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcxY2sgJtMYD"
      },
      "outputs": [],
      "source": [
        "def concatenate_lyrics(df, artist_name):\n",
        "    # Filter the DataFrame for the given artist\n",
        "    artist_df = df[df['Artist'] == artist_name]\n",
        "\n",
        "    # Concatenate all lyrics into a single string\n",
        "    all_lyrics = '\\n'.join(artist_df['Lyric'])\n",
        "\n",
        "    return all_lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1sHYt-7tTQJ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/all_albums/dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Prompt the user for an artist name\n",
        "artist_name = input(\"Enter the name of the artist: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4PHeAd4tU-x"
      },
      "outputs": [],
      "source": [
        "# Get concatenated lyrics\n",
        "lyrics = concatenate_lyrics(df, artist_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rec_Z2jjs_U"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(' '.join(sorted(set(lyrics))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eoIAEBW-GZj"
      },
      "outputs": [],
      "source": [
        "pattern = r'\\[.*?\\]'\n",
        "lyrics = re.sub(pattern, '', lyrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4IMH9p6AADC"
      },
      "outputs": [],
      "source": [
        "pattern = r'\\((chorus|CHORUS|verse|VERSE|intro|INTRO)(.*?)\\)'\n",
        "lyrics =  re.sub(pattern, '', lyrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSRdHDkVAtfO"
      },
      "outputs": [],
      "source": [
        "def special_symbols_resolver(s):\n",
        "    replacements = {\n",
        "        'à': 'a', 'á': 'a', 'â': 'a', 'ã': 'a', 'ä': 'a',\n",
        "        'ç': 'c',\n",
        "        'ö': 'o',\n",
        "        'ú': 'u', 'ü': 'u',\n",
        "        'œ': 'oe',\n",
        "        'Â': 'A',\n",
        "        '‰': '', '™': '', '´': '', '·': '', '¦': '', '': '', '': '',\n",
        "        '˜': '', '“': '', '†': '', '…': '', '′': '', '″': '', '�': '',\n",
        "        'í': 'i', 'é': 'e', 'ï': 'i', 'ó': 'o', ';': ',', '‘': '\\'', '’': '\\'', ':': ',', 'е': 'e'\n",
        "    }\n",
        "\n",
        "    for symbol, replacement in replacements.items():\n",
        "        s = s.replace(symbol, replacement)\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aORde0HHA0wt"
      },
      "outputs": [],
      "source": [
        "lyrics = special_symbols_resolver(lyrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlcOUEqCjwx1"
      },
      "outputs": [],
      "source": [
        "replace_with_space = ['\\u2005', '\\u200b', '\\u205f', '\\xa0', '-']\n",
        "replace_letters = {}\n",
        "remove_list = ['\\)', '\\(', '–','\"','”', '\"', '\\[.*\\]', '.*\\|.*', '—', '(VERSE)', '(CHORUS ONE)']\n",
        "\n",
        "cleaned_lyrics = lyrics\n",
        "\n",
        "for string in remove_list:\n",
        "    cleaned_lyrics = re.sub(string,'',cleaned_lyrics)\n",
        "for string in replace_with_space:\n",
        "    cleaned_lyrics = re.sub(string,' ',cleaned_lyrics)\n",
        "print(''.join(sorted(set(cleaned_lyrics))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu25J3UI-jvt"
      },
      "outputs": [],
      "source": [
        "print(lyrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWz1im9pj16b"
      },
      "outputs": [],
      "source": [
        "split_point = int(len(cleaned_lyrics)*0.95)\n",
        "train_data = cleaned_lyrics[:split_point]\n",
        "test_data = cleaned_lyrics[split_point:]\n",
        "train_data_seg = []\n",
        "for i in range(0, len(train_data), 500):\n",
        "        text = train_data[i:min(i+500, len(train_data))]\n",
        "        train_data_seg.append(text)\n",
        "train_data_seg = Dataset.from_dict({'text':train_data_seg})\n",
        "print(len(train_data_seg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DATURjuCk-VT"
      },
      "outputs": [],
      "source": [
        "# Loading the model with double quantization\n",
        "model_name = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n",
        "# model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
        "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bguSmLJ1lQYK"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikbQM38BlRCz"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def generate_lyrics(query, model):\n",
        "    encoding = tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "    generation_config = GenerationConfig(max_new_tokens=250, pad_token_id = tokenizer.eos_token_id,repetition_penalty=1.3, eos_token_id = tokenizer.eos_token_id)\n",
        "    outputs = model.generate(input_ids=encoding.input_ids, generation_config=generation_config)\n",
        "    text_output = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
        "    print('INPUT\\n', query, '\\n\\nOUTPUT\\n', text_output[len(query):])\n",
        "generate_lyrics(test_data[500:1000], model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1749GT7lSu8"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_alpha = 32\n",
        "lora_dropout = 0.05\n",
        "lora_rank = 32\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_rank,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\")\n",
        "\n",
        "peft_model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXc-7O09lVX5"
      },
      "outputs": [],
      "source": [
        "output_dir = \"Tkrogg/TinyLlama_taylor_swift\"\n",
        "per_device_train_batch_size = 3\n",
        "gradient_accumulation_steps = 2\n",
        "optim = \"paged_adamw_32bit\"\n",
        "save_strategy=\"steps\"\n",
        "save_steps = 10\n",
        "logging_steps = 10\n",
        "learning_rate = 2e-3\n",
        "max_grad_norm = 0.3\n",
        "max_steps = 200\n",
        "warmup_ratio = 0.03\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    push_to_hub=True,\n",
        "    report_to='wandb'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcXeyuuylYvs"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=train_data_seg,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=500,\n",
        "    dataset_text_field='text',\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments\n",
        ")\n",
        "peft_model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y3cgYnYlaW5"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEpSGKxNpt_u"
      },
      "outputs": [],
      "source": [
        "generate_lyrics(test_data[500:1000], model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
